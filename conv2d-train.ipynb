{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54fd7ba9-8836-4190-924a-9f12250334a2",
   "metadata": {},
   "source": [
    "### conv2d-train\n",
    "ref: http://int-info.com/PyLearn/PyLearnKeras04.html\n",
    "\n",
    "Mar05, 2023, ms\n",
    "\n",
    "This is to upload GitHub."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9668d59f",
   "metadata": {},
   "source": [
    "##### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ad36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "#import matplotlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Flatten, Conv2D\n",
    "from keras import initializers, callbacks\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35b6420b",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e7557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interporate data\n",
    "def interporateAxisData(ax, x_points):\n",
    "    \"\"\"\n",
    "    ax: ndarray, values from single axis\n",
    "    x_points: points to interporate\n",
    "    \"\"\"\n",
    "    # squeeze original x range into (1 to x_points)\n",
    "    ori_x_scaled = np.array([(x_points/len(ax)) * i\n",
    "                                            for i in range(1, len(ax)+1)])\n",
    "    # linear range 1, 2,,, x_points\n",
    "    new_x = np.array([i for i in range(1, x_points+1)])\n",
    "    # interporate function\n",
    "    myfunc = interp1d(ori_x_scaled, ax, fill_value=\"extrapolate\")\n",
    "    # return interporated ax values\n",
    "    return myfunc(new_x)\n",
    "\n",
    "\n",
    "def interporateData(reads, x_points):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    read_count = len(reads)  # eq. to csv count\n",
    "    # axis count.  get it from the first read\n",
    "    ax_count = reads[0].shape[1]\n",
    "\n",
    "    # prepare a bucket with the same sape of reads and\n",
    "    # override this with interporated data axis by axis\n",
    "    reads_bucket = []\n",
    "\n",
    "    for read_idx in range(read_count):\n",
    "        temp_read = np.zeros((x_points, ax_count))\n",
    "        for ax_idx in range(ax_count):\n",
    "            temp_read[:, ax_idx] = interporateAxisData(\n",
    "                                        reads[read_idx][:, ax_idx], x_points\n",
    "                                        )\n",
    "        reads_bucket.append(temp_read)\n",
    "\n",
    "    return reads_bucket\n",
    "\n",
    "\n",
    "# collect data from all csv files in given dir\n",
    "def prepareLabelDicts(labels):\n",
    "    \"\"\"\n",
    "    labels is a list of strings with a lot of duplicates\n",
    "    create following dicts\n",
    "    label2idx\n",
    "    idx2label\n",
    "    \"\"\"\n",
    "    label2idx = {l: idx for idx, l in enumerate(sorted(list(set(labels))))}\n",
    "    # flip key value. Values are all unique (no replicates).\n",
    "    idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "    return label2idx, idx2label\n",
    "\n",
    "\n",
    "def parseAllCSVs(csv_dir):\n",
    "    \"\"\"\n",
    "    raw csv cols\n",
    "    0: time stamp\n",
    "    1-3: acc x, y, z <--- take these and,\n",
    "    4-6: gyr x, y, z <--- these\n",
    "    7: A1, touch to read\n",
    "    8: A3, touch to stop\n",
    "    \"\"\"\n",
    "    reads = []\n",
    "    labels = []\n",
    "    for csv in glob.glob(os.path.normpath(os.path.join(csv_dir, '*.csv'))):\n",
    "        reads.append(np.loadtxt(csv, delimiter=',', usecols=range(1, 7)))\n",
    "        labels.append(os.path.basename(csv).split('-')[0])\n",
    "\n",
    "    return reads, labels  # list of ndarray and a list\n",
    "\n",
    "\n",
    "def normalizeMinMax(array_x):\n",
    "    \"\"\"\n",
    "    min max normalization\n",
    "    values will be packed into 0 to 1\n",
    "    \"\"\"\n",
    "    return (array_x - array_x.min()) / (array_x.max() - array_x.min())\n",
    "\n",
    "\n",
    "def normalizeData(reads):\n",
    "    \"\"\"\n",
    "    this is used for interporated reads in which read shape is the same for\n",
    "    all reads in given reads\n",
    "    \"\"\"\n",
    "    read_count = len(reads)  # eq. to csv count\n",
    "    # get the read shape from the first read\n",
    "    x_points, ax_count = reads[0].shape\n",
    "\n",
    "    # as done in another func, prepare a bucket with the same sape of reads and\n",
    "    # override this with interporated data axis by axis\n",
    "    reads_bucket = []\n",
    "\n",
    "    for read_idx in range(read_count):\n",
    "        temp_read = np.zeros((x_points, ax_count))\n",
    "        for ax_idx in range(ax_count):\n",
    "            temp_read[:, ax_idx] = normalizeMinMax(\n",
    "                                        reads[read_idx][:, ax_idx]\n",
    "                                        )\n",
    "        reads_bucket.append(temp_read)\n",
    "\n",
    "    return reads_bucket\n",
    "\n",
    "\n",
    "# save file related\n",
    "def createOutFileName(head, ext, ts, test_size, batch_size, epochs, test_acc):\n",
    "    out_name = head\n",
    "    out_name += \"-ts\" + str(test_size)\n",
    "    out_name += \"-bs\" + str(batch_size)\n",
    "    out_name += \"-es\" + str(epochs)\n",
    "    out_name += \"-val_acc\" + \"{:.4f}\".format(test_acc)\n",
    "    out_name += \"-\" + ts + ext\n",
    "    return out_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "559e9c08",
   "metadata": {},
   "source": [
    "#### main story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813aa68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# parse csv file data into a list of ndarray\n",
    "#\n",
    "\n",
    "#data_dir = './data-pen-alphabet2/'  # raw reads\n",
    "data_dir = './data-a-n-Feb27-2023-chopped-Mar05-2023/'\n",
    "#data_dir = 'PATH TO THE CSV FILE DIR'\n",
    "reads, labels = parseAllCSVs(data_dir)\n",
    "\n",
    "# prepare label handling dicts\n",
    "label2idx, idx2label = prepareLabelDicts(labels)\n",
    "# one hot label\n",
    "one_hot_labels = to_categorical(\n",
    "                    np.array([label2idx[l] for l in labels]),\n",
    "                    dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5ed9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'ho', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ka', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ke', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ku', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mo', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'mu', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ne', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'ni', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'nu', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 'ra', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 're', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ri', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ro', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'sa', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'se', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'su', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'ta', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'te', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'ti', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'tu', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wa', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'wo', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'ya', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yo', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu', 'yu']\n"
     ]
    }
   ],
   "source": [
    "# ちょい見\n",
    "print(len(reads))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c49407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# interporate data\n",
    "#\n",
    "reads_itp = interporateData(reads, x_points=30)\n",
    "\n",
    "#\n",
    "# normalization\n",
    "#\n",
    "reads_itp_norm = normalizeData(reads_itp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# draw 3 graphs\n",
    "# check before after interporation and normalization\n",
    "# ---------------------------------------------------\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3.5*3))\n",
    "spec = fig.add_gridspec(nrows=3, ncols=1)\n",
    "\n",
    "ax_raw = fig.add_subplot(spec[0, 0])\n",
    "ax_itp = fig.add_subplot(spec[1, 0])\n",
    "ax_itp_norm = fig.add_subplot(spec[2, 0])\n",
    "\n",
    "# choose what you want to see\n",
    "# if you want to see 'ho' data,\n",
    "# use labels.find('ho') to find the first read_idx for 'ho'\n",
    "read_idx = 105  # ho\n",
    "ax_idx = 2  # acc_z\n",
    "\n",
    "char = labels[read_idx]\n",
    "ax_names = [\n",
    "    'acc_x', 'acc_y', 'acc_z',\n",
    "    'gyr_x', 'gyr_y', 'gyr_z'\n",
    "]\n",
    "ax_name = ax_names[ax_idx]\n",
    "\n",
    "suptitle = 'raw->interporated->normalized graph check: '\n",
    "suptitle += char + ', ' + ax_name\n",
    "fig.suptitle(suptitle)\n",
    "\n",
    "ax_raw.plot(reads[read_idx][:, ax_idx], label='raw')\n",
    "ax_itp.plot(reads_itp[read_idx][:, ax_idx], label='itp')\n",
    "ax_itp_norm.plot(reads_itp_norm[read_idx][:, ax_idx], label='itp-norm')\n",
    "\n",
    "leg_raw = ax_raw.legend()\n",
    "leg_itp = ax_itp.legend()\n",
    "leg_itp_norm = ax_itp_norm.legend()\n",
    "\n",
    "out = 'read_idx-' + str(read_idx)\n",
    "out += '-' + char + '-' + ax_names[ax_idx] + '.png'\n",
    "plt.savefig(out)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534d0d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(920, 30, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# re-shaping for cnn\n",
    "#\n",
    "'''\n",
    "now reads_itp_norm is a list of (30=datapoints, 6=num_of_sensor_axis) ndarray.\n",
    "to pour this data into cnn, it should have a shape something like\n",
    "(num_of_reads, datapoints, num_of_sensor_axis, channel=1)\n",
    "'''\n",
    "datapoints, sensor_axes = reads_itp_norm[0].shape\n",
    "# prepare zero bucket\n",
    "reads_cnn = np.zeros((len(reads_itp_norm), datapoints, sensor_axes, 1))\n",
    "for j in range(len(reads_itp_norm)):\n",
    "    reads_cnn[j, :, :, 0] = reads_itp_norm[j]\n",
    "\n",
    "print(reads_cnn.shape)\n",
    "# print(reads_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# see 6 axis data as an image\n",
    "# This is to see one read data.\n",
    "# ------------------------------\n",
    "read_idx = 105  # ho\n",
    "\n",
    "char = labels[read_idx]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "\n",
    "suptitle = '6 axis in an image: '\n",
    "suptitle += char\n",
    "fig.suptitle(suptitle)\n",
    "\n",
    "plt.imshow(reads_cnn[read_idx, :, :, 0].T, cmap='gray')\n",
    "\n",
    "out = 'read_idx-' + str(read_idx) + '-' + char + '-6x30.png'\n",
    "plt.savefig(out)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d8c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# batch plotting, batch saving\n",
    "# print 6 axis image from all 20 reads for all hiragana!\n",
    "# -------------------------------------------------------\n",
    "# ref: https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html\n",
    "\n",
    "for idx, char in idx2label.items():\n",
    "    fig, axs = plt.subplots(2, 10, sharey=True)\n",
    "\n",
    "    fig.suptitle('image-ed data: ' + char)\n",
    "    out = './image-ed-data-' + char + '.png'\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(10):\n",
    "            #print('i:j=', i, j, (k*20+i*10)+j)\n",
    "            axs[i, j].imshow(reads_cnn[(idx*20+i*10)+j, :, :, 0], cmap='gray')\n",
    "\n",
    "    plt.savefig(out)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9148aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (920, 30, 6, 1)\n",
      "y.shape (920, 46)\n",
      "x_train.shape (736, 30, 6, 1)\n",
      "x_test.shape (184, 30, 6, 1)\n",
      "y_train.shape (736, 46)\n",
      "y_test.shape (184, 46)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# split train and test\n",
    "#\n",
    "\n",
    "# -------------------\n",
    "test_size = 0.2\n",
    "# -------------------\n",
    "\n",
    "x = reads_cnn\n",
    "y = one_hot_labels\n",
    "\n",
    "print('x.shape', x.shape)\n",
    "print('y.shape', y.shape)\n",
    "# print(y)\n",
    "\n",
    "# split!\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=test_size, stratify=y,\n",
    "    #random_state=11\n",
    "    )\n",
    "\n",
    "print('x_train.shape', x_train.shape)\n",
    "print('x_test.shape', x_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1cd8090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_filter (Conv2D)        (None, 25, 6, 4)          28        \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 600)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 46)                27646     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,674\n",
      "Trainable params: 27,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# creating CNN\n",
    "#\n",
    "\n",
    "input_size1 = 30  # data points used in the interporation\n",
    "input_size2 = 6  # axis number from LSM6D\n",
    "num_class = len(label2idx)  # isn't it?\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# change here to play around\n",
    "kernel_size = (6, 1)\n",
    "strides = (1, 1)\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=4, kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation='relu',\n",
    "        input_shape=(input_size1, input_size2, 1),\n",
    "        kernel_initializer=initializers.TruncatedNormal(),\n",
    "        name='conv_filter'\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_class, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(),\n",
    "                metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4133114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "16/16 [==============================] - 10s 29ms/step - loss: 3.8142 - accuracy: 0.0462 - val_loss: 3.7578 - val_accuracy: 0.2554\n",
      "Epoch 2/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.7046 - accuracy: 0.2867 - val_loss: 3.6037 - val_accuracy: 0.6196\n",
      "Epoch 3/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 3.5274 - accuracy: 0.4674 - val_loss: 3.3925 - val_accuracy: 0.7500\n",
      "Epoch 4/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 3.2974 - accuracy: 0.7160 - val_loss: 3.1361 - val_accuracy: 0.8207\n",
      "Epoch 5/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 3.0214 - accuracy: 0.7677 - val_loss: 2.8421 - val_accuracy: 0.9076\n",
      "Epoch 6/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.7119 - accuracy: 0.8655 - val_loss: 2.5238 - val_accuracy: 0.9348\n",
      "Epoch 7/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.3892 - accuracy: 0.8777 - val_loss: 2.1978 - val_accuracy: 0.9728\n",
      "Epoch 8/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.0556 - accuracy: 0.9226 - val_loss: 1.8752 - val_accuracy: 0.9511\n",
      "Epoch 9/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.7341 - accuracy: 0.9429 - val_loss: 1.5721 - val_accuracy: 0.9511\n",
      "Epoch 10/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.4373 - accuracy: 0.9457 - val_loss: 1.3001 - val_accuracy: 0.9783\n",
      "Epoch 11/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1763 - accuracy: 0.9592 - val_loss: 1.0644 - val_accuracy: 0.9783\n",
      "Epoch 12/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.9608 - accuracy: 0.9647 - val_loss: 0.8581 - val_accuracy: 0.9674\n",
      "Epoch 13/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.7658 - accuracy: 0.9620 - val_loss: 0.7111 - val_accuracy: 0.9674\n",
      "Epoch 14/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.6292 - accuracy: 0.9647 - val_loss: 0.5784 - val_accuracy: 0.9783\n",
      "Epoch 15/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.5039 - accuracy: 0.9674 - val_loss: 0.4859 - val_accuracy: 0.9783\n",
      "Epoch 16/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.4148 - accuracy: 0.9755 - val_loss: 0.3867 - val_accuracy: 0.9783\n",
      "Epoch 17/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.3379 - accuracy: 0.9715 - val_loss: 0.3363 - val_accuracy: 0.9728\n",
      "Epoch 18/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.2838 - accuracy: 0.9728 - val_loss: 0.2912 - val_accuracy: 0.9674\n",
      "Epoch 19/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.2404 - accuracy: 0.9701 - val_loss: 0.2341 - val_accuracy: 0.9783\n",
      "Epoch 20/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.2007 - accuracy: 0.9851 - val_loss: 0.2129 - val_accuracy: 0.9728\n",
      "Epoch 21/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.1724 - accuracy: 0.9823 - val_loss: 0.1841 - val_accuracy: 0.9728\n",
      "Epoch 22/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.1491 - accuracy: 0.9864 - val_loss: 0.1748 - val_accuracy: 0.9674\n",
      "Epoch 23/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.1313 - accuracy: 0.9810 - val_loss: 0.1378 - val_accuracy: 0.9837\n",
      "Epoch 24/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.1135 - accuracy: 0.9837 - val_loss: 0.1405 - val_accuracy: 0.9783\n",
      "Epoch 25/150\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0953 - accuracy: 0.9864 - val_loss: 0.1302 - val_accuracy: 0.9728\n",
      "Epoch 26/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0892 - accuracy: 0.9823 - val_loss: 0.1167 - val_accuracy: 0.9728\n",
      "Epoch 27/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0784 - accuracy: 0.9918 - val_loss: 0.1063 - val_accuracy: 0.9783\n",
      "Epoch 28/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0724 - accuracy: 0.9878 - val_loss: 0.1136 - val_accuracy: 0.9783\n",
      "Epoch 29/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0597 - accuracy: 0.9918 - val_loss: 0.0952 - val_accuracy: 0.9783\n",
      "Epoch 30/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0597 - accuracy: 0.9851 - val_loss: 0.1074 - val_accuracy: 0.9674\n",
      "Epoch 31/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0480 - accuracy: 0.9905 - val_loss: 0.0940 - val_accuracy: 0.9783\n",
      "Epoch 32/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0479 - accuracy: 0.9891 - val_loss: 0.0838 - val_accuracy: 0.9837\n",
      "Epoch 33/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0417 - accuracy: 0.9932 - val_loss: 0.0915 - val_accuracy: 0.9728\n",
      "Epoch 34/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0363 - accuracy: 0.9918 - val_loss: 0.0662 - val_accuracy: 0.9837\n",
      "Epoch 35/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0334 - accuracy: 0.9946 - val_loss: 0.0548 - val_accuracy: 0.9946\n",
      "Epoch 36/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0302 - accuracy: 0.9973 - val_loss: 0.0615 - val_accuracy: 0.9837\n",
      "Epoch 37/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0271 - accuracy: 0.9946 - val_loss: 0.0685 - val_accuracy: 0.9837\n",
      "Epoch 38/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0259 - accuracy: 0.9973 - val_loss: 0.0680 - val_accuracy: 0.9837\n",
      "Epoch 39/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 0.9783\n",
      "Epoch 40/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0194 - accuracy: 0.9986 - val_loss: 0.0561 - val_accuracy: 0.9946\n",
      "Epoch 41/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0170 - accuracy: 0.9986 - val_loss: 0.0485 - val_accuracy: 0.9946\n",
      "Epoch 42/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0172 - accuracy: 0.9986 - val_loss: 0.0673 - val_accuracy: 0.9837\n",
      "Epoch 43/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0141 - accuracy: 0.9986 - val_loss: 0.0479 - val_accuracy: 0.9946\n",
      "Epoch 44/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 0.9783\n",
      "Epoch 45/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 0.9837\n",
      "Epoch 46/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0541 - val_accuracy: 0.9837\n",
      "Epoch 47/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 0.9783\n",
      "Epoch 48/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9891\n",
      "Epoch 49/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 0.9783\n",
      "Epoch 50/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0425 - val_accuracy: 0.9946\n",
      "Epoch 51/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0454 - val_accuracy: 0.9837\n",
      "Epoch 52/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0067 - accuracy: 0.9986 - val_loss: 0.0367 - val_accuracy: 0.9891\n",
      "Epoch 53/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9946\n",
      "Epoch 54/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9837\n",
      "Epoch 55/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0460 - val_accuracy: 0.9891\n",
      "Epoch 56/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 0.9891\n",
      "Epoch 57/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 0.9837\n",
      "Epoch 58/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 0.9946\n",
      "Epoch 59/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 0.9837\n",
      "Epoch 60/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 0.9891\n",
      "Epoch 61/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 0.9837\n",
      "Epoch 62/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 0.9946\n",
      "Epoch 63/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 0.9946\n",
      "Epoch 64/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0339 - val_accuracy: 0.9891\n",
      "Epoch 65/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 0.9891\n",
      "Epoch 66/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 0.9946\n",
      "Epoch 67/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0329 - val_accuracy: 0.9946\n",
      "Epoch 68/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 0.9946\n",
      "Epoch 69/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 0.9946\n",
      "Epoch 70/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 0.9891\n",
      "Epoch 71/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 9.7390e-04 - accuracy: 1.0000 - val_loss: 0.0288 - val_accuracy: 0.9946\n",
      "Epoch 72/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.6390e-04 - accuracy: 1.0000 - val_loss: 0.0328 - val_accuracy: 0.9891\n",
      "Epoch 73/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 0.9891\n",
      "Epoch 74/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.0486e-04 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 0.9946\n",
      "Epoch 75/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.9262e-04 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 0.9946\n",
      "Epoch 76/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.7683e-04 - accuracy: 1.0000 - val_loss: 0.0319 - val_accuracy: 0.9946\n",
      "Epoch 77/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.2069e-04 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 0.9946\n",
      "Epoch 78/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.8606e-04 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 0.9946\n",
      "Epoch 79/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.4728e-04 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 0.9946\n",
      "Epoch 80/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.0580e-04 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 0.9946\n",
      "Epoch 81/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.2544e-04 - accuracy: 1.0000 - val_loss: 0.0408 - val_accuracy: 0.9891\n",
      "Epoch 82/150\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 6.3006e-04 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 0.9946\n",
      "Epoch 83/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.8796e-04 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9891\n",
      "Epoch 84/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.5204e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9946\n",
      "Epoch 85/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.9746e-04 - accuracy: 1.0000 - val_loss: 0.0308 - val_accuracy: 0.9891\n",
      "Epoch 86/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.0495e-04 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 0.9946\n",
      "Epoch 87/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.9267e-04 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 0.9946\n",
      "Epoch 88/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.5393e-04 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 0.9946\n",
      "Epoch 89/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.9824e-04 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 0.9946\n",
      "Epoch 90/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.5905e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9946\n",
      "Epoch 91/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.7659e-04 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 0.9891\n",
      "Epoch 92/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.3092e-04 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 0.9946\n",
      "Epoch 93/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.7218e-04 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 0.9946\n",
      "Epoch 94/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.0112e-04 - accuracy: 1.0000 - val_loss: 0.0202 - val_accuracy: 0.9946\n",
      "Epoch 95/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.6680e-04 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 0.9946\n",
      "Epoch 96/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.0472e-04 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 0.9891\n",
      "Epoch 97/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.7535e-04 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 0.9891\n",
      "Epoch 98/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.1648e-04 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 0.9891\n",
      "Epoch 99/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.7898e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9946\n",
      "Epoch 100/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.6190e-04 - accuracy: 1.0000 - val_loss: 0.0243 - val_accuracy: 0.9946\n",
      "Epoch 101/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.5472e-05 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 0.9946\n",
      "Epoch 102/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5272e-04 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 0.9946\n",
      "Epoch 103/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.1791e-05 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9946\n",
      "Epoch 104/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.0111e-05 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 0.9837\n",
      "Epoch 105/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.2495e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9946\n",
      "Epoch 106/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.3132e-05 - accuracy: 1.0000 - val_loss: 0.0259 - val_accuracy: 0.9946\n",
      "Epoch 107/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.8836e-05 - accuracy: 1.0000 - val_loss: 0.0192 - val_accuracy: 0.9946\n",
      "Epoch 108/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.3297e-05 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 0.9946\n",
      "Epoch 109/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.7529e-05 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 0.9946\n",
      "Epoch 110/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 5.1160e-05 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9946\n",
      "Epoch 111/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.2833e-05 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 0.9946\n",
      "Epoch 112/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 4.6598e-05 - accuracy: 1.0000 - val_loss: 0.0189 - val_accuracy: 0.9946\n",
      "Epoch 113/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.2111e-05 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 0.9946\n",
      "Epoch 114/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0170e-04 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 0.9946\n",
      "Epoch 115/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.8127e-05 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 0.9946\n",
      "Epoch 116/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.0209e-05 - accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 0.9946\n",
      "Epoch 117/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.5608e-05 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 0.9946\n",
      "Epoch 118/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.3216e-05 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9946\n",
      "Epoch 119/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 4.4768e-05 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 0.9946\n",
      "Epoch 120/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.8643e-05 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9946\n",
      "Epoch 121/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.8159e-05 - accuracy: 1.0000 - val_loss: 0.0265 - val_accuracy: 0.9946\n",
      "Epoch 122/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.2739e-05 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 0.9946\n",
      "Epoch 123/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1347e-05 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 0.9946\n",
      "Epoch 124/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.4329e-05 - accuracy: 1.0000 - val_loss: 0.0282 - val_accuracy: 0.9946\n",
      "Epoch 125/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 4.2897e-05 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 0.9946\n",
      "Epoch 126/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.4966e-06 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 0.9946\n",
      "Epoch 127/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 9.4561e-06 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 0.9946\n",
      "Epoch 128/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 3.0093e-05 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 0.9946\n",
      "Epoch 129/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.2283e-06 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9946\n",
      "Epoch 130/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 9.1446e-06 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 0.9946\n",
      "Epoch 131/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.6747e-05 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 0.9946\n",
      "Epoch 132/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.6267e-06 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 0.9946\n",
      "Epoch 133/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.4392e-06 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9946\n",
      "Epoch 134/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.6200e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9946\n",
      "Epoch 135/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 5.0536e-06 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 0.9946\n",
      "Epoch 136/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.8412e-06 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9946\n",
      "Epoch 137/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.6151e-06 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 0.9946\n",
      "Epoch 138/150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.2701e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9946\n",
      "Epoch 139/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.9771e-06 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 0.9946\n",
      "Epoch 140/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.2016e-06 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 0.9946\n",
      "Epoch 141/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.3487e-06 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9946\n",
      "Epoch 142/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.4093e-06 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 0.9946\n",
      "Epoch 143/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.6888e-06 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9946\n",
      "Epoch 144/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.4017e-06 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 0.9946\n",
      "Epoch 145/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.5576e-06 - accuracy: 1.0000 - val_loss: 0.0252 - val_accuracy: 0.9946\n",
      "Epoch 146/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.3986e-06 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 0.9946\n",
      "Epoch 147/150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 5.7359e-06 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9946\n",
      "Epoch 148/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.4595e-06 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 0.9946\n",
      "Epoch 149/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.9095e-06 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9946\n",
      "Epoch 150/150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.7163e-06 - accuracy: 1.0000 - val_loss: 0.0210 - val_accuracy: 0.9946\n"
     ]
    }
   ],
   "source": [
    "# this is for tensorboard\n",
    "# dir with ts\n",
    "postfix = '-kernel61-strides11-filter4'\n",
    "ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_folder = \"logs/fit/\" + ts + postfix\n",
    "shutil.rmtree(log_folder, ignore_errors=True)\n",
    "\n",
    "tsb_callbacks = callbacks.TensorBoard(log_dir=log_folder, histogram_freq=1)\n",
    "\n",
    "# ---------------\n",
    "batch_size = 46  # set this to the char count for now\n",
    "epochs = 150\n",
    "# ---------------\n",
    "\n",
    "#\n",
    "# TRAINING!\n",
    "#\n",
    "learning_process = model.fit(\n",
    "                                x_train, y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs,\n",
    "                                validation_data=(x_test, y_test),\n",
    "                                callbacks= [tsb_callbacks],\n",
    "                                verbose=1\n",
    "                            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b85b2ba5",
   "metadata": {},
   "source": [
    "#### post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b09973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0210 - accuracy: 0.9946\n",
      "hiragana-model-CNN-ts0.2-bs46-es150-val_acc0.9946-20230306-210944-kernel61-strides11-filter4.h5 saved.\n",
      "idx2label-ts0.2-bs46-es150-val_acc0.9946-20230306-210944-kernel61-strides11-filter4.pkl saved.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# saving files\n",
    "#\n",
    "\n",
    "# model ---------------------------------------------------------------------\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "ts += postfix\n",
    "\n",
    "model_name = createOutFileName(\n",
    "    \"hiragana-model-CNN\", \".h5\", ts, test_size, batch_size, epochs, test_acc)\n",
    "model.save(model_name)\n",
    "print(model_name, \"saved.\")\n",
    "\n",
    "# idx2labels dict -----------------------------------------------------------\n",
    "# pkl it and save\n",
    "i2l_pkl = createOutFileName(\n",
    "    \"idx2label\", \".pkl\", ts, test_size, batch_size, epochs, test_acc)\n",
    "with open(i2l_pkl, 'wb', -1) as PKL:\n",
    "    pickle.dump(idx2label, PKL)\n",
    "print(i2l_pkl, \"saved.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e44bb2f6",
   "metadata": {},
   "source": [
    "#### NOT IN USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82846c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# confusion matrix ----------------------------------------------------------\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "cm = tf.math.confusion_matrix(\n",
    "    y_test.argmax(axis=1),  # y_test is in one-hot expression\n",
    "    y_pred.argmax(axis=1),  # y_pred is array of probabilities\n",
    "    dtype=tf.dtypes.int32\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b838651fa23bdc229f6724f63068b4328d808deb39f80760d5f78e010c311c3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
